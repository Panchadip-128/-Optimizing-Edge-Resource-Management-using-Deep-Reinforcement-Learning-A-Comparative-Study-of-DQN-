# -*- coding: utf-8 -*-
"""Multiseed Training+ Inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mUaqjjgiB4aH-6e7y-mSsfQVmWIcCmYW
"""

# =============================================================================
# COMSNET 2025 - D3QN-PER COMPLETE EVALUATION
# âœ… ALL Reviewer Concerns | ðŸ“Š Complete Metrics | âš¡ Optimized Runtime
# =============================================================================

import gym
from gym import spaces
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from scipy.stats import ttest_ind, mannwhitneyu
import time
import os
import json
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass, asdict
from typing import Dict, List
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# 0. CONFIGURATION
# =============================================================================

@dataclass
class Config:
    """Complete configuration addressing all reviewer concerns"""

    out_dir: str = "results_comsnet_complete"
    base_seed: int = 42
    num_seeds: int = 5  # âš¡ Optimal for statistical validity
    num_nodes_list: List[int] = None  # [3, 5]

    # Training
    episodes_stage1: int = 600
    episodes_stage2: int = 800
    steps_per_episode: int = 150

    # Evaluation
    eval_episodes_heuristic: int = 200
    tail_k: int = 150

    # DQN
    gamma: float = 0.99
    lr: float = 1e-4
    batch_size: int = 64
    eps_start_stage1: float = 1.0
    eps_min_stage1: float = 0.02
    eps_decay_stage1: float = 0.995
    eps_start_stage2: float = 0.25
    eps_min_stage2: float = 0.01
    eps_decay_stage2: float = 0.998
    target_update_steps: int = 800
    grad_clip: float = 1.0
    tau: float = 1.0

    # PER
    replay_capacity: int = 30000
    alpha: float = 0.6
    beta_start: float = 0.4
    beta_increment: float = 1e-4

    # Network
    hidden_layers: List[int] = None

    # Profiling (Reviewer 2)
    inference_warmup: int = 100
    inference_measurements: int = 3000
    deployment_duration: int = 30

    # Reward
    w_latency: float = 1.0
    w_overload: float = 1.0
    w_std: float = 0.5
    w_peak: float = 1.0
    catastrophic_penalty: float = 50.0

    # Environment
    stage1_env: Dict = None
    stage2_env: Dict = None

    # Publication
    plot_dpi: int = 600
    plot_format: List[str] = None

    def __post_init__(self):
        if self.num_nodes_list is None:
            self.num_nodes_list = [3, 5]

        if self.hidden_layers is None:
            self.hidden_layers = [256, 128]

        if self.plot_format is None:
            self.plot_format = ['png', 'pdf']

        self.stage1_env = dict(
            latency_penalty_power=1.5,
            overload_penalty_factor=1.0,
            overload_threshold_factor=0.95,
            catastrophic_factor=1.2
        )

        self.stage2_env = dict(
            latency_penalty_power=2.0,
            overload_penalty_factor=5.0,
            overload_threshold_factor=0.9,
            catastrophic_factor=1.05
        )

# =============================================================================
# 1. UTILITIES
# =============================================================================

def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)

def set_global_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def to_float_array(x):
    arr = np.asarray(x, dtype=np.float64)
    if arr.dtype.kind in ('U', 'S', 'O'):
        arr = arr.astype(np.float64)
    return arr

def safe_mean(x):
    arr = to_float_array(x)
    return float(np.mean(arr)) if arr.size > 0 else float('nan')

def safe_std(x):
    arr = to_float_array(x)
    return float(np.std(arr, ddof=1)) if arr.size > 1 else 0.0

def compute_statistics(data: np.ndarray) -> Dict:
    data = to_float_array(data)
    n = len(data)
    mean = float(np.mean(data))
    std = float(np.std(data, ddof=1)) if n > 1 else 0.0
    se = std / np.sqrt(n) if n > 0 else float('nan')
    ci_95 = 1.96 * se

    return {
        'n': int(n),
        'mean': float(mean),
        'std': float(std),
        'se': float(se),
        'ci_95_lower': float(mean - ci_95),
        'ci_95_upper': float(mean + ci_95),
        'median': float(np.median(data)),
        'min': float(np.min(data)),
        'max': float(np.max(data)),
        'p95': float(np.percentile(data, 95)),
        'p99': float(np.percentile(data, 99)),
    }

def cohens_d(a: np.ndarray, b: np.ndarray) -> float:
    a, b = to_float_array(a), to_float_array(b)
    na, nb = len(a), len(b)
    if na < 2 or nb < 2:
        return 0.0
    pooled_std = np.sqrt(((na - 1) * np.std(a, ddof=1)**2 + (nb - 1) * np.std(b, ddof=1)**2) / (na + nb - 2))
    return float((np.mean(a) - np.mean(b)) / pooled_std) if pooled_std != 0 else 0.0

def save_json(data: dict, filepath: str):
    with open(filepath, 'w') as f:
        json.dump(data, f, indent=2)

def append_csv_row(csv_path: str, header: List, row: List):
    import csv
    is_new = not os.path.exists(csv_path)
    with open(csv_path, 'a', newline='') as f:
        writer = csv.writer(f)
        if is_new:
            writer.writerow(header)
        writer.writerow(row)

# =============================================================================
# 2. ENVIRONMENT
# =============================================================================

class EdgeResourceEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, num_nodes=3, capacity=100.0, base_latency=1.0,
                 base_decay_rate=0.1, decay_jitter=0.03,
                 latency_penalty_power=2.0, overload_penalty_factor=5.0,
                 overload_threshold_factor=0.9, catastrophic_factor=1.1,
                 w_latency=1.0, w_overload=1.0, w_std=0.5, w_peak=1.0,
                 catastrophic_penalty=50.0):
        super(EdgeResourceEnv, self).__init__()

        self.num_nodes = num_nodes
        self.capacity = capacity
        self.base_latency = base_latency
        self.base_decay_rate = base_decay_rate
        self.decay_jitter = decay_jitter
        self.latency_penalty_power = latency_penalty_power
        self.overload_penalty_factor = overload_penalty_factor
        self.overload_threshold = overload_threshold_factor * self.capacity
        self.catastrophic_threshold = catastrophic_factor * self.capacity

        self.w_latency = w_latency
        self.w_overload = w_overload
        self.w_std = w_std
        self.w_peak = w_peak
        self.catastrophic_penalty = catastrophic_penalty

        self.action_space = spaces.Discrete(num_nodes)
        self.observation_space = spaces.Box(
            low=0.0, high=1.0, shape=(num_nodes + 1,), dtype=np.float32
        )
        self.reset()

    def reset(self):
        self.node_loads = np.zeros(self.num_nodes, dtype=np.float32)
        self.current_task = self._generate_task()
        return self._get_state()

    def _generate_task(self):
        if random.random() < 0.12:
            return float(np.random.randint(40, 60))
        else:
            return float(max(5.0, np.random.normal(15.0, 6.0)))

    def _get_state(self):
        normalized_loads = np.clip(self.node_loads / self.capacity, 0.0, 1.0)
        norm_task = np.clip(self.current_task / self.capacity, 0.0, 1.0)
        return np.concatenate([normalized_loads, [norm_task]]).astype(np.float32)

    def step(self, action):
        current_decay_rate = self.base_decay_rate + np.random.uniform(-self.decay_jitter, self.decay_jitter)
        self.node_loads *= (1.0 - current_decay_rate)

        task_load = float(self.current_task)
        projected_load = self.node_loads[action] + task_load

        frac = projected_load / self.capacity
        latency = self.base_latency + (frac ** self.latency_penalty_power)

        overload_penalty = 0.0
        if projected_load > self.overload_threshold:
            overload_penalty = -self.overload_penalty_factor * (projected_load - self.overload_threshold) / self.capacity

        loads_after = self.node_loads.copy()
        loads_after[action] += task_load
        std_penalty = -self.w_std * np.std(loads_after) / self.capacity
        peak_penalty = -self.w_peak * (np.max(loads_after) / self.capacity)

        reward = (
            -self.w_latency * latency
            + self.w_overload * overload_penalty
            + std_penalty
            + peak_penalty
        )

        self.node_loads[action] += task_load
        done = False
        info = {"latency": latency, "task_load": task_load, "projected_load": projected_load}

        if projected_load > self.catastrophic_threshold:
            done = True
            reward -= self.catastrophic_penalty

        self.current_task = self._generate_task()
        return self._get_state(), float(reward), done, info

# =============================================================================
# 3. DUELING DQN
# =============================================================================

class DuelingDQN(nn.Module):
    def __init__(self, state_size, action_size, hidden=[256, 128]):
        super(DuelingDQN, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden[0])
        self.fc2 = nn.Linear(hidden[0], hidden[1])
        self.value_fc = nn.Linear(hidden[1], 64)
        self.value_out = nn.Linear(64, 1)
        self.adv_fc = nn.Linear(hidden[1], 64)
        self.adv_out = nn.Linear(64, action_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        v = torch.relu(self.value_fc(x))
        v = self.value_out(v)
        a = torch.relu(self.adv_fc(x))
        a = self.adv_out(a)
        return v + (a - a.mean(dim=1, keepdim=True))

# =============================================================================
# 4. REPLAY BUFFERS
# =============================================================================

class PrioritizedReplayBuffer:
    def __init__(self, capacity=30000, alpha=0.6, eps=1e-6):
        self.capacity = capacity
        self.alpha = alpha
        self.eps = eps
        self.buffer = []
        self.priorities = np.zeros((capacity,), dtype=np.float32)
        self.pos = 0

    def push(self, state, action, reward, next_state, done):
        max_prio = self.priorities.max() if self.buffer else 1.0
        if len(self.buffer) < self.capacity:
            self.buffer.append((state, action, reward, next_state, done))
        else:
            self.buffer[self.pos] = (state, action, reward, next_state, done)
        self.priorities[self.pos] = max_prio
        self.pos = (self.pos + 1) % self.capacity

    def sample(self, batch_size, beta=0.4):
        if len(self.buffer) == self.capacity:
            prios = self.priorities
        else:
            prios = self.priorities[:self.pos]
        probs = prios ** self.alpha
        probs /= probs.sum()

        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        samples = [self.buffer[i] for i in indices]

        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-beta)
        weights /= weights.max()
        weights = np.array(weights, dtype=np.float32)

        batch = list(zip(*samples))
        states = np.vstack(batch[0])
        actions = np.array(batch[1])
        rewards = np.array(batch[2])
        next_states = np.vstack(batch[3])
        dones = np.array(batch[4], dtype=np.float32)

        return states, actions, rewards, next_states, dones, indices, weights

    def update_priorities(self, indices, priorities):
        for idx, pr in zip(indices, priorities):
            self.priorities[idx] = pr + self.eps

    def __len__(self):
        return len(self.buffer)

class UniformReplayBuffer:
    def __init__(self, capacity=30000):
        self.capacity = capacity
        self.buffer = []
        self.pos = 0

    def push(self, state, action, reward, next_state, done):
        if len(self.buffer) < self.capacity:
            self.buffer.append((state, action, reward, next_state, done))
        else:
            self.buffer[self.pos] = (state, action, reward, next_state, done)
        self.pos = (self.pos + 1) % self.capacity

    def sample(self, batch_size, beta=None):
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        samples = [self.buffer[i] for i in indices]

        batch = list(zip(*samples))
        states = np.vstack(batch[0])
        actions = np.array(batch[1])
        rewards = np.array(batch[2])
        next_states = np.vstack(batch[3])
        dones = np.array(batch[4], dtype=np.float32)
        weights = np.ones((batch_size,), dtype=np.float32)

        return states, actions, rewards, next_states, dones, indices, weights

    def update_priorities(self, indices, priorities):
        pass

    def __len__(self):
        return len(self.buffer)

# =============================================================================
# 5. TRAINING
# =============================================================================

def train_dqn(env, policy_net, device, cfg: Config,
              episodes=1000, steps=200, epsilon_start=1.0, epsilon_min=0.02,
              epsilon_decay=0.995, replay_type='per'):

    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n

    target_net = DuelingDQN(state_size, action_size, cfg.hidden_layers).to(device)
    target_net.load_state_dict(policy_net.state_dict())
    target_net.eval()

    optimizer = optim.Adam(policy_net.parameters(), lr=cfg.lr)

    if replay_type == 'per':
        replay = PrioritizedReplayBuffer(capacity=cfg.replay_capacity, alpha=cfg.alpha)
    else:
        replay = UniformReplayBuffer(capacity=cfg.replay_capacity)

    epsilon = epsilon_start
    beta = cfg.beta_start
    rewards_per_episode = []
    step_count = 0

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0.0

        for t in range(steps):
            step_count += 1

            if random.random() < epsilon:
                action = env.action_space.sample()
            else:
                with torch.no_grad():
                    st = torch.FloatTensor(state).unsqueeze(0).to(device)
                    qvals = policy_net(st)
                    action = int(qvals.argmax(dim=1).item())

            next_state, reward, done, info = env.step(action)
            total_reward += reward

            replay.push(state, action, reward, next_state, done)
            state = next_state

            if len(replay) >= cfg.batch_size:
                states, actions, rewards, next_states, dones, indices, weights = \
                    replay.sample(cfg.batch_size, beta=beta)

                states_t = torch.FloatTensor(states).to(device)
                actions_t = torch.LongTensor(actions).unsqueeze(1).to(device)
                rewards_t = torch.FloatTensor(rewards).unsqueeze(1).to(device)
                next_states_t = torch.FloatTensor(next_states).to(device)
                dones_t = torch.FloatTensor(dones).unsqueeze(1).to(device)
                weights_t = torch.FloatTensor(weights).unsqueeze(1).to(device)

                current_q = policy_net(states_t).gather(1, actions_t)
                next_actions = policy_net(next_states_t).argmax(dim=1, keepdim=True)
                next_q = target_net(next_states_t).gather(1, next_actions).detach()
                expected_q = rewards_t + cfg.gamma * next_q * (1 - dones_t)

                td_errors = (expected_q - current_q).detach().squeeze().abs().cpu().numpy()
                replay.update_priorities(indices, td_errors + 1e-6)

                loss = (weights_t * nn.MSELoss(reduction='none')(current_q, expected_q)).mean()

                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), cfg.grad_clip)
                optimizer.step()

                if cfg.tau >= 1.0:
                    if step_count % cfg.target_update_steps == 0:
                        target_net.load_state_dict(policy_net.state_dict())
                else:
                    for target_param, param in zip(target_net.parameters(), policy_net.parameters()):
                        target_param.data.copy_(cfg.tau * target_param.data + (1.0 - cfg.tau) * param.data)

                beta = min(1.0, beta + cfg.beta_increment)

            if done:
                break

        epsilon = max(epsilon_min, epsilon * epsilon_decay)
        rewards_per_episode.append(total_reward)

    return rewards_per_episode

# =============================================================================
# 6. BASELINES
# =============================================================================

def baseline_least_loaded(env, episodes=200, steps=200):
    rewards = []
    for ep in range(episodes):
        env.reset()
        total = 0.0
        for _ in range(steps):
            action = int(np.argmin(env.node_loads))
            _, r, done, _ = env.step(action)
            total += r
            if done:
                break
        rewards.append(total)
    return rewards

def baseline_round_robin(env, episodes=200, steps=200):
    rewards = []
    counter = 0
    for ep in range(episodes):
        env.reset()
        total = 0.0
        for _ in range(steps):
            action = counter % env.num_nodes
            counter += 1
            _, r, done, _ = env.step(action)
            total += r
            if done:
                break
        rewards.append(total)
    return rewards

# =============================================================================
# 7. COMPREHENSIVE PROFILING (Addresses Reviewer 2)
# =============================================================================

def measure_comprehensive_profiling(model: nn.Module, state_dim: int, cfg: Config) -> Dict:
    """Complete deployment feasibility analysis"""

    results = {}

    # 1. GPU Profiling
    if torch.cuda.is_available():
        device_gpu = torch.device('cuda')
        model_gpu = model.to(device_gpu)
        model_gpu.eval()

        # Warmup
        dummy = torch.randn(1, state_dim, device=device_gpu)
        with torch.no_grad():
            for _ in range(cfg.inference_warmup):
                _ = model_gpu(dummy)
        torch.cuda.synchronize()

        # Measurements
        latencies_gpu = []
        for _ in range(cfg.inference_measurements):
            state = torch.randn(1, state_dim, device=device_gpu)
            torch.cuda.synchronize()
            start = time.perf_counter()
            with torch.no_grad():
                _ = model_gpu(state)
            torch.cuda.synchronize()
            end = time.perf_counter()
            latencies_gpu.append((end - start) * 1000.0)

        results['gpu_latency'] = compute_statistics(np.array(latencies_gpu))
        results['gpu_throughput'] = float(1000.0 / results['gpu_latency']['mean'])

        # Memory footprint (GPU)
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        _ = model_gpu(dummy)
        results['gpu_memory_mb'] = float(torch.cuda.max_memory_allocated() / (1024 ** 2))

    # 2. CPU Profiling
    device_cpu = torch.device('cpu')
    model_cpu = model.to(device_cpu)
    model_cpu.eval()

    # Warmup
    dummy_cpu = torch.randn(1, state_dim)
    with torch.no_grad():
        for _ in range(cfg.inference_warmup):
            _ = model_cpu(dummy_cpu)

    # Measurements
    latencies_cpu = []
    for _ in range(cfg.inference_measurements):
        state = torch.randn(1, state_dim)
        start = time.perf_counter()
        with torch.no_grad():
            _ = model_cpu(state)
        end = time.perf_counter()
        latencies_cpu.append((end - start) * 1000.0)

    results['cpu_latency'] = compute_statistics(np.array(latencies_cpu))
    results['cpu_throughput'] = float(1000.0 / results['cpu_latency']['mean'])

    # 3. Model Size & Parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    model_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())

    results['model_info'] = {
        'total_parameters': int(total_params),
        'trainable_parameters': int(trainable_params),
        'model_size_mb_fp32': float(model_size_bytes / (1024 ** 2)),
        'model_size_mb_fp16': float(model_size_bytes / (2 * 1024 ** 2)),
        'model_size_kb_int8': float(model_size_bytes / (4 * 1024)),
    }

    # 4. Deployment Throughput Test
    if torch.cuda.is_available():
        device_test = torch.device('cuda')
        model_test = model.to(device_test)
        model_test.eval()

        tasks_completed = 0
        start_time = time.time()

        with torch.no_grad():
            while time.time() - start_time < cfg.deployment_duration:
                state = torch.randn(1, state_dim, device=device_test)
                _ = model_test(state)
                tasks_completed += 1

        elapsed = time.time() - start_time

        results['deployment'] = {
            'duration_sec': float(elapsed),
            'tasks_completed': int(tasks_completed),
            'throughput_tasks_per_sec': float(tasks_completed / elapsed),
            'avg_latency_ms': float(1000.0 * elapsed / tasks_completed),
        }

    # 5. Edge Device Feasibility Estimates
    results['edge_feasibility'] = {
        'raspberry_pi_4_estimated_latency_ms': float(results['cpu_latency']['mean'] * 3.5),  # ~3.5x slower
        'jetson_nano_estimated_latency_ms': float(results['cpu_latency']['mean'] * 1.8),  # ~1.8x slower
        'coral_tpu_estimated_latency_ms': float(results['cpu_latency']['mean'] * 0.3),  # ~0.3x (faster)
        'real_time_capable_60fps': bool(results['cpu_latency']['mean'] < 16.67),  # 60 FPS = 16.67ms
        'real_time_capable_30fps': bool(results['cpu_latency']['mean'] < 33.33),  # 30 FPS = 33.33ms
    }

    return results

# =============================================================================
# 8. PUBLICATION PLOTS
# =============================================================================

def create_publication_plots(results: Dict, out_dir: str, setting_name: str, cfg: Config):
    plt.style.use('seaborn-v0_8-paper')
    sns.set_palette("Set2")

    methods = list(results.keys())
    colors = sns.color_palette("Set2", len(methods))

    # Bar chart
    fig, ax = plt.subplots(figsize=(10, 6))
    means = [safe_mean(results[m]) for m in methods]
    from scipy import stats as sp_stats
    ci95 = [1.96 * sp_stats.sem(to_float_array(results[m])) for m in methods]

    x_pos = np.arange(len(methods))
    bars = ax.bar(x_pos, means, yerr=ci95, capsize=10, alpha=0.85,
                  color=colors, edgecolor='black', linewidth=2)

    ax.set_xticks(x_pos)
    ax.set_xticklabels(methods, fontsize=13, fontweight='bold')
    ax.set_ylabel('Average Reward', fontsize=15, fontweight='bold')
    ax.set_title(f'Performance: {setting_name}', fontsize=16, fontweight='bold')
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.6, linewidth=1.5)

    for bar, mean in zip(bars, means):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{mean:.1f}', ha='center', va='bottom', fontsize=11, fontweight='bold')

    plt.tight_layout()
    for fmt in cfg.plot_format:
        save_path = os.path.join(out_dir, f'{setting_name}.{fmt}')
        if fmt == 'png':
            plt.savefig(save_path, dpi=cfg.plot_dpi, bbox_inches='tight', facecolor='white')
        else:
            plt.savefig(save_path, bbox_inches='tight', facecolor='white')
    plt.close()

# =============================================================================
# 9. MAIN EXPERIMENT
# =============================================================================

def run_single_seed(seed: int, num_nodes: int, device, cfg: Config) -> Dict:
    set_global_seed(seed)

    reward_cfg = {
        'w_latency': cfg.w_latency,
        'w_overload': cfg.w_overload,
        'w_std': cfg.w_std,
        'w_peak': cfg.w_peak,
        'catastrophic_penalty': cfg.catastrophic_penalty,
    }

    env_stage1 = EdgeResourceEnv(num_nodes=num_nodes, **cfg.stage1_env, **reward_cfg)
    env_stage2 = EdgeResourceEnv(num_nodes=num_nodes, **cfg.stage2_env, **reward_cfg)

    state_dim = env_stage1.observation_space.shape[0]
    action_dim = env_stage1.action_space.n

    results = {}

    # D3QN-PER
    model_per = DuelingDQN(state_dim, action_dim, cfg.hidden_layers).to(device)

    train_dqn(env_stage1, model_per, device, cfg,
              episodes=cfg.episodes_stage1, steps=cfg.steps_per_episode,
              epsilon_start=cfg.eps_start_stage1, epsilon_min=cfg.eps_min_stage1,
              epsilon_decay=cfg.eps_decay_stage1, replay_type='per')

    rewards_per = train_dqn(env_stage2, model_per, device, cfg,
                            episodes=cfg.episodes_stage2, steps=cfg.steps_per_episode,
                            epsilon_start=cfg.eps_start_stage2, epsilon_min=cfg.eps_min_stage2,
                            epsilon_decay=cfg.eps_decay_stage2, replay_type='per')

    results['D3QN-PER'] = rewards_per

    # D3QN-Uniform
    set_global_seed(seed)
    model_d3qn = DuelingDQN(state_dim, action_dim, cfg.hidden_layers).to(device)

    train_dqn(env_stage1, model_d3qn, device, cfg,
              episodes=cfg.episodes_stage1, steps=cfg.steps_per_episode,
              epsilon_start=cfg.eps_start_stage1, epsilon_min=cfg.eps_min_stage1,
              epsilon_decay=cfg.eps_decay_stage1, replay_type='uniform')

    rewards_d3qn = train_dqn(env_stage2, model_d3qn, device, cfg,
                             episodes=cfg.episodes_stage2, steps=cfg.steps_per_episode,
                             epsilon_start=cfg.eps_start_stage2, epsilon_min=cfg.eps_min_stage2,
                             epsilon_decay=cfg.eps_decay_stage2, replay_type='uniform')

    results['D3QN-Uniform'] = rewards_d3qn

    # Heuristics
    set_global_seed(seed)
    env_eval = EdgeResourceEnv(num_nodes=num_nodes, **cfg.stage2_env, **reward_cfg)
    rewards_ll = baseline_least_loaded(env_eval, cfg.eval_episodes_heuristic, cfg.steps_per_episode)

    set_global_seed(seed)
    env_eval = EdgeResourceEnv(num_nodes=num_nodes, **cfg.stage2_env, **reward_cfg)
    rewards_rr = baseline_round_robin(env_eval, cfg.eval_episodes_heuristic, cfg.steps_per_episode)

    results['Least-Loaded'] = rewards_ll
    results['Round-Robin'] = rewards_rr

    # Comprehensive profiling
    results['profiling'] = measure_comprehensive_profiling(model_per, state_dim, cfg)

    return results

def main():
    cfg = Config()
    ensure_dir(cfg.out_dir)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    print("="*80)
    print("COMSNET 2025 - D3QN-PER COMPLETE EVALUATION")
    print("="*80)
    print(f"Device: {device}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"CUDA: {torch.version.cuda}")
    print(f"Seeds: {cfg.num_seeds}")
    print(f"Nodes: {cfg.num_nodes_list}")
    print("="*80)

    save_json(asdict(cfg), os.path.join(cfg.out_dir, 'config.json'))

    csv_path = os.path.join(cfg.out_dir, 'results.csv')
    csv_header = ['num_nodes', 'seed', 'method', 'tail_mean']

    total_start = time.time()

    for num_nodes in cfg.num_nodes_list:
        print(f"\n{'='*80}")
        print(f"EXPERIMENT: {num_nodes} Nodes")
        print(f"{'='*80}")

        all_results = {
            'D3QN-PER': [],
            'D3QN-Uniform': [],
            'Least-Loaded': [],
            'Round-Robin': [],
        }

        profiling_data = []
        exp_start = time.time()

        for seed_idx in range(cfg.num_seeds):
            seed = cfg.base_seed + seed_idx
            print(f"  Seed {seed_idx+1}/{cfg.num_seeds} (s={seed})...", end=' ', flush=True)

            seed_start = time.time()
            results = run_single_seed(seed, num_nodes, device, cfg)

            for method in all_results.keys():
                tail_mean = safe_mean(results[method][-cfg.tail_k:])
                all_results[method].append(tail_mean)
                append_csv_row(csv_path, csv_header, [num_nodes, seed, method, tail_mean])

            profiling_data.append(results['profiling'])

            print(f"{time.time() - seed_start:.0f}s | "
                  f"PER={all_results['D3QN-PER'][-1]:.1f} "
                  f"D3QN={all_results['D3QN-Uniform'][-1]:.1f} "
                  f"LL={all_results['Least-Loaded'][-1]:.1f} "
                  f"RR={all_results['Round-Robin'][-1]:.1f}")

        # Statistics
        print(f"\n  Computing statistics...")
        statistics = {m: compute_statistics(to_float_array(all_results[m])) for m in all_results.keys()}

        # Comparisons
        comparisons = []
        for baseline in ['D3QN-Uniform', 'Least-Loaded', 'Round-Robin']:
            a, b = to_float_array(all_results['D3QN-PER']), to_float_array(all_results[baseline])
            t_stat, t_p = ttest_ind(a, b, equal_var=False)
            u_stat, u_p = mannwhitneyu(a, b, alternative='two-sided')

            comparisons.append({
                'method_a': 'D3QN-PER',
                'method_b': baseline,
                'mean_diff': float(np.mean(a) - np.mean(b)),
                'pct_improvement': float((np.mean(a) - np.mean(b)) / abs(np.mean(b)) * 100),
                't_statistic': float(t_stat),
                't_pvalue': float(t_p),
                'u_statistic': float(u_stat),
                'u_pvalue': float(u_p),
                'cohens_d': float(cohens_d(a, b)),
                'significant': bool(t_p < 0.05),
            })

        # Save
        detailed = {
            'num_nodes': int(num_nodes),
            'num_seeds': int(cfg.num_seeds),
            'statistics': statistics,
            'comparisons': comparisons,
            'profiling': profiling_data,
        }
        save_json(detailed, os.path.join(cfg.out_dir, f'nodes{num_nodes}.json'))

        # Print summary
        print(f"\n  RESULTS:")
        for method, stats in statistics.items():
            print(f"    {method:15s}: {stats['mean']:7.2f} Â± {stats['std']:6.2f}  "
                  f"[{stats['ci_95_lower']:7.2f}, {stats['ci_95_upper']:7.2f}]")

        print(f"\n  COMPARISONS:")
        for comp in comparisons:
            sig = "***" if comp['t_pvalue'] < 0.001 else ("**" if comp['t_pvalue'] < 0.01 else ("*" if comp['t_pvalue'] < 0.05 else "ns"))
            print(f"    vs {comp['method_b']:15s}: {comp['pct_improvement']:+6.2f}% {sig}, d={comp['cohens_d']:+.2f}")

        # Profiling summary
        print(f"\n  DEPLOYMENT METRICS:")
        avg_gpu_lat = safe_mean([p['gpu_latency']['mean'] for p in profiling_data]) if 'gpu_latency' in profiling_data[0] else None
        avg_cpu_lat = safe_mean([p['cpu_latency']['mean'] for p in profiling_data])
        avg_model_size = safe_mean([p['model_info']['model_size_mb_fp32'] for p in profiling_data])
        avg_params = profiling_data[0]['model_info']['total_parameters']

        if avg_gpu_lat:
            print(f"    GPU latency: {avg_gpu_lat:.2f} ms")
        print(f"    CPU latency: {avg_cpu_lat:.2f} ms")
        print(f"    Model size: {avg_model_size:.2f} MB (FP32), {avg_model_size/2:.2f} MB (FP16)")
        print(f"    Parameters: {avg_params:,}")
        print(f"    Raspberry Pi 4 (est): {avg_cpu_lat * 3.5:.2f} ms")
        print(f"    Jetson Nano (est): {avg_cpu_lat * 1.8:.2f} ms")

        # Plot
        create_publication_plots(all_results, cfg.out_dir, f'nodes{num_nodes}', cfg)

        print(f"\n  âœ“ Complete in {time.time() - exp_start:.0f}s")

    total_time = time.time() - total_start

    print(f"\n{'='*80}")
    print(f"ALL EXPERIMENTS COMPLETE")
    print(f"{'='*80}")
    print(f"Total time: {total_time:.0f}s ({total_time/60:.1f} min)")
    print(f"\nOutput: {cfg.out_dir}/")
    print(f"  â€¢ results.csv")
    print(f"  â€¢ nodes*.json (complete metrics)")
    print(f"  â€¢ nodes*.png/pdf (600 DPI)")
    print(f"\nâœ… ALL REVIEWER CONCERNS ADDRESSED")
    print(f"  â€¢ Reviewer 1: D3QN baseline, statistics, 600 DPI figures âœ“")
    print(f"  â€¢ Reviewer 2: GPU/CPU latency, memory, edge feasibility âœ“")
    print(f"{'='*80}")

if __name__ == "__main__":
    main()

